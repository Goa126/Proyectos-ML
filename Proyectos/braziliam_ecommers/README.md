# üáßüá∑ An√°lisis Integral de E-commerce Brasile√±o (Olist Dataset)

Este proyecto realiza un an√°lisis exhaustivo de datos de comercio electr√≥nico de Olist (2016-2018), abarcando desde la ingenier√≠a de datos y estructuraci√≥n en SQL, hasta el an√°lisis exploratorio, clustering de clientes y modelos predictivos de Machine Learning.

## üöÄ Objetivo del Proyecto
Transformar datos crudos de transacciones, log√≠stica y rese√±as en insights estrat√©gicos para mejorar la experiencia del cliente y la eficiencia operativa. El an√°lisis busca responder preguntas clave sobre **estacionalidad de ventas**, **causas de insatisfacci√≥n** y **segmentaci√≥n de clientes**.

## üìÇ Estructura del Proyecto

```bash
braziliam_ecommers/
‚îú‚îÄ‚îÄ data/                       # Dataset original (archivos CSV de Olist)
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter Notebooks con el an√°lisis y modelado
‚îÇ   ‚îú‚îÄ‚îÄ analisis_series_temporales.ipynb  # EDA, Series Temporales, Clustering y ML Predictivo
‚îÇ   ‚îî‚îÄ‚îÄ sistemas_de_recomendacion.ipynb   # (En desarrollo) Motores de recomendaci√≥n
‚îú‚îÄ‚îÄ sql/                        # Scripts SQL para la base de datos
‚îÇ   ‚îî‚îÄ‚îÄ sql_braziliam.sql       # Schema, PKs, FKs y creaci√≥n de Vistas Anal√≠ticas
‚îú‚îÄ‚îÄ src/                        # C√≥digo fuente modular (funciones auxiliares)
‚îú‚îÄ‚îÄ models/                     # Modelos entrenados serializados (pkl/joblib)
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias del proyecto
‚îî‚îÄ‚îÄ .env                        # Variables de entorno (credenciales de BD)
```

## üõ†Ô∏è Tecnolog√≠as Utilizadas
*   **Lenguaje:** Python 3.12+
*   **Base de Datos:** PostgreSQL
*   **Librer√≠as Principales:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`, `sqlalchemy`, `python-dotenv`.

## üìä Flujo de Trabajo y Metodolog√≠a

### 1. Ingenier√≠a de Datos (SQL & PostgreSQL)
*   **Modelado de Datos:** Se estructur√≥ la base de datos relacional definiendo Llaves Primarias (PK) y For√°neas (FK) para garantizar la integridad referencial.
*   **Vistas Anal√≠ticas:** Creaci√≥n de `vista_entrenamiento_ml` y tablas enriquecidas (`ds_enriquecido_ml`) para consolidar informaci√≥n dispersa (pedidos, clientes, productos, pagos y rese√±as) en una √∫nica fuente de verdad para los modelos.

### 2. An√°lisis Exploratorio y Series Temporales
*   **Conexi√≥n Directa:** Integraci√≥n de SQL con Python mediante `SQLAlchemy` para consultas eficientes.
*   **Tendencias Temporales:** Detecci√≥n de patrones de crecimiento org√°nico en 2017 y picos estacionales (Black Friday).
*   **Comportamiento del Usuario:** Identificaci√≥n de horarios "Prime Time" de compra (d√≠as laborales 10:00 - 17:00) y ca√≠da de actividad en fines de semana.

### 3. Segmentaci√≥n de Clientes (Clustering)
Uso de algoritmos no supervisados (**K-Means**) para clasificar la experiencia de entrega en 3 clusters:
*   **Cluster 0 (Problema de Producto):** Entregas r√°pidas pero baja calificaci√≥n (problemas de calidad en Muebles/Telefon√≠a).
*   **Cluster 1 (Riesgo Log√≠stico):** Demoras extremas (>30 d√≠as) y p√©sima calificaci√≥n.
*   **Cluster 2 (Est√°ndar de Oro):** Entregas eficientes y satisfacci√≥n alta.

### 4. Machine Learning: Predicci√≥n de Insatisfacci√≥n
Entrenamiento de un modelo de **Random Forest Classifier** para predecir si un cliente tendr√° una experiencia negativa (Review Score < 3).
*   **Desempe√±o:** Recall del 54% (detecta m√°s de la mitad de las quejas potenciales).
*   **Hallazgos Clave:**
    *   **D√≠as de Entrega Real:** El factor m√°s cr√≠tico.
    *   **Costo del Flete:** Sorpresivamente, un flete caro genera m√°s insatisfacci√≥n que un producto caro.
    *   **Dimensiones:** Productos voluminosos tienen mayor tasa de incidencia.

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone <url-del-repositorio>
    cd braziliam_ecommers
    ```

2.  **Crear entorno virtual e instalar dependencias:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Configurar Variables de Entorno:**
    Crear un archivo `.env` en la ra√≠z con las credenciales de PostgreSQL:
    ```env
    DB_USER=tu_usuario
    DB_PASSWORD=tu_contrase√±a
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=nombre_base_datos
    ```

4.  **Ejecutar Notebooks:**
    Iniciar Jupyter Lab o Notebook para explorar `notebooks/analisis_series_temporales.ipynb`.

## üìà Pr√≥ximos Pasos
*   Desarrollo de un **Sistema de Recomendaci√≥n** h√≠brido (Collaborative Filtering + Content-Based) en `sistemas_de_recomendacion.ipynb`.
*   Despliegue del modelo predictivo como API.

---
*Autor: [Tu Nombre]*
